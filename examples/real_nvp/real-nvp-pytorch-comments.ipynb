{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook found at https://github.com/senya-ashukha/real-nvp-pytorch but with additional comments as I read the Real NVP paper (https://arxiv.org/pdf/1605.08803.pdf) and learn PyTorch syntax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 8\n",
    "rcParams['figure.dpi'] = 300\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import distributions\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVP(nn.Module): # base class Module \n",
    "    def __init__(self, nets, nett, mask, prior):\n",
    "        super(RealNVP, self).__init__()\n",
    "        \n",
    "        self.prior = prior\n",
    "        self.mask = nn.Parameter(mask, requires_grad=False)\n",
    "        self.t = torch.nn.ModuleList([nett() for _ in range(len(masks))]) # translation function (net) \n",
    "        self.s = torch.nn.ModuleList([nets() for _ in range(len(masks))]) # scaling function (net)\n",
    "        # nn.ModuleList is just like a Python list. It was designed to store any desired number of nn.Module’s.\n",
    "        \n",
    "    def g(self, z):\n",
    "        x = z\n",
    "        for i in range(len(self.t)): # for each layer \n",
    "            x_ = x*self.mask[i] # splitting features between channels. \n",
    "                                # features selected here used to compute s(x) and f(x) but not updated themselves yet.  \n",
    "            s = self.s[i](x_)*(1 - self.mask[i]) \n",
    "            t = self.t[i](x_)*(1 - self.mask[i])\n",
    "            x = x_ + (1 - self.mask[i]) * (x * torch.exp(s) + t)\n",
    "        return x\n",
    "\n",
    "    def f(self, x):\n",
    "        log_det_J, z = x.new_zeros(x.shape[0]), x\n",
    "        # new_zeros(size) returns a Tensor of size size filled with 0 \n",
    "        for i in reversed(range(len(self.t))): # move backwards through layers \n",
    "            z_ = self.mask[i] * z # tensor of size num samples x num features\n",
    "            s = self.s[i](z_) * (1-self.mask[i]) # self.s[i] is the entire sequence of scaling operations\n",
    "            t = self.t[i](z_) * (1-self.mask[i])\n",
    "            z = (1 - self.mask[i]) * (z - t) * torch.exp(-s) + z_\n",
    "            log_det_J -= s.sum(dim=1)\n",
    "            # each pass through here applies all operations defined in nets() and all the ones defined in nett() \n",
    "        # self.s[1](z_) is not the same as self.s[3](z_)\n",
    "        return z, log_det_J\n",
    "    \n",
    "    def log_prob(self,x):\n",
    "        z, logp = self.f(x) # z = f(x) \n",
    "        return self.prior.log_prob(z) + logp\n",
    "        \n",
    "    def sample(self, batchSize): \n",
    "        z = self.prior.sample((batchSize, 1))\n",
    "        logp = self.prior.log_prob(z)\n",
    "        x = self.g(z)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create net s and net t \n",
    "nets = lambda: nn.Sequential(nn.Linear(2, 256), nn.LeakyReLU(), nn.Linear(256, 256), nn.LeakyReLU(), nn.Linear(256, 2), nn.Tanh())\n",
    "nett = lambda: nn.Sequential(nn.Linear(2, 256), nn.LeakyReLU(), nn.Linear(256, 256), nn.LeakyReLU(), nn.Linear(256, 2)) \n",
    "#    nn.Linear(256, 256) means 256 input features, 256 output features \n",
    "\n",
    "masks = torch.from_numpy(np.array([[0, 1], [1, 0]] * 3).astype(np.float32)) # 6x2 matrix. len(masks) = 6 = num subblocks. \n",
    "# so we have a total of 3 neural blocks (see fig. 1 of boltzmann generators paper)\n",
    "prior = distributions.MultivariateNormal(torch.zeros(2), torch.eye(2))\n",
    "flow = RealNVP(nets, nett, masks, prior) # define our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: loss = 2.478\n",
      "iter 500: loss = 0.520\n"
     ]
    }
   ],
   "source": [
    "# torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, \n",
    "#    amsgrad=False): Implements Adam algorithm.\n",
    "\n",
    "optimizer = torch.optim.Adam([p for p in flow.parameters() if p.requires_grad==True], lr=1e-4)\n",
    "for t in range(5000):    \n",
    "    noisy_moons = datasets.make_moons(n_samples=100, noise=.05)[0].astype(np.float32) # Make two interleaving half circles\n",
    "    loss = -flow.log_prob(torch.from_numpy(noisy_moons)).mean()\n",
    "    \n",
    "    optimizer.zero_grad() # we need to set the gradients to zero before starting to do \n",
    "                          # backpropragation because PyTorch accumulates the gradients on \n",
    "                          # subsequent backward passes.\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if t % 500 == 0:\n",
    "        print('iter %s:' % t, 'loss = %.3f' % loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets.make_moons(): Make two interleaving half circles\n",
    "# torch.from_numpy(ndarray) → Tensor: Creates a Tensor from a numpy.ndarray\n",
    "\n",
    "# plot data points in latent space\n",
    "noisy_moons = datasets.make_moons(n_samples=1000, noise=.05)[0].astype(np.float32)\n",
    "# index [0] to skip returning the class labels Y \n",
    "z = flow.f(torch.from_numpy(noisy_moons))[0].detach().numpy()\n",
    "plt.subplot(221)\n",
    "plt.scatter(z[:, 0], z[:, 1])\n",
    "plt.title(r'$z = f(X)$')\n",
    "\n",
    "# sample from latent distribution \n",
    "z = np.random.multivariate_normal(np.zeros(2), np.eye(2), 1000)\n",
    "plt.subplot(222)\n",
    "plt.scatter(z[:, 0], z[:, 1])\n",
    "plt.title(r'$z \\sim p(z)$')\n",
    "\n",
    "# plot actual data\n",
    "plt.subplot(223) \n",
    "x = datasets.make_moons(n_samples=1000, noise=.05)[0].astype(np.float32)\n",
    "plt.scatter(x[:, 0], x[:, 1], c='r')\n",
    "plt.title(r'$X \\sim p(X)$')\n",
    "\n",
    "# plot transformation of the points sampled in latent space\n",
    "plt.subplot(224) \n",
    "x = flow.sample(1000).detach().numpy() \n",
    "plt.scatter(x[:, 0, 0], x[:, 0, 1], c='r')\n",
    "plt.title(r'$X = g(z)$')\n",
    "\n",
    "# use of tensor.detach(): https://discuss.pytorch.org/t/clone-and-detach-in-v0-4-0/16861/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
