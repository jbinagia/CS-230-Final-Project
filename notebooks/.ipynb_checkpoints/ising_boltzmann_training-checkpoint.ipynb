{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "import torch\n",
    "from torch import distributions\n",
    "from torch import nn\n",
    "from sklearn import datasets\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project.models.ising import IsingModel\n",
    "import project.networks.net as net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Ising Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 139 µs, sys: 1e+03 ns, total: 140 µs\n",
      "Wall time: 144 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.600000000000001"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h = 0.0\n",
    "J = 0.8\n",
    "T = 1.0\n",
    "N = 8\n",
    "\n",
    "ising = IsingModel(h = h, J = J)\n",
    "x0 = ising.init_coords(N)\n",
    "\n",
    "ising.draw_config(x0)\n",
    "%time ising.energy(x0) # energy of a given configuration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training set\n",
    "We'd like to have a $m$ x $N$ matrix containing our training data where $m$ is the number of realizations of the system and $N$ is the number of features (i.e. the flattened dimensions of the system). For example, a training set with 1000 samples of the Ising Model for $N=8$ would be of size (1000, 64). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 2 # as done in Noe et al. for the biwell potential \n",
    "flattened_size = N**2\n",
    "\n",
    "training_set = np.zeros((num_samples,flattened_size), dtype=np.float32)\n",
    "for i in range(num_samples):\n",
    "    training_set[i,:] = ising.init_coords(N).flatten() # generate random configuration \n",
    "training_set = torch.from_numpy(training_set) # convert to PyTorch tensor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an aside, note that although we flatten our configurations for training, the flattening procedure can easily be reversed via the `rehsape()` function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1,  1, -1,  1,  1,  1,  1,  1],\n",
       "       [ 1,  1, -1, -1, -1, -1,  1,  1],\n",
       "       [ 1,  1, -1,  1, -1, -1, -1, -1],\n",
       "       [-1, -1, -1, -1,  1, -1,  1, -1],\n",
       "       [-1,  1, -1,  1,  1, -1,  1,  1],\n",
       "       [-1, -1,  1, -1,  1,  1,  1, -1],\n",
       "       [-1, -1, -1,  1,  1,  1, -1,  1],\n",
       "       [ 1, -1, -1,  1, -1,  1, -1,  1]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1,  1, -1,  1,  1,  1,  1,  1],\n",
       "       [ 1,  1, -1, -1, -1, -1,  1,  1],\n",
       "       [ 1,  1, -1,  1, -1, -1, -1, -1],\n",
       "       [-1, -1, -1, -1,  1, -1,  1, -1],\n",
       "       [-1,  1, -1,  1,  1, -1,  1,  1],\n",
       "       [-1, -1,  1, -1,  1,  1,  1, -1],\n",
       "       [-1, -1, -1,  1,  1,  1, -1,  1],\n",
       "       [ 1, -1, -1,  1, -1,  1, -1,  1]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x0).flatten().reshape((N,N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boltzmann generator\n",
    "## Define network architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = lambda: nn.Sequential(nn.Linear(N**2, 256), nn.LeakyReLU(), nn.Linear(256, 256), nn.LeakyReLU(), nn.Linear(256, N**2), nn.Tanh()) # net s\n",
    "nett = lambda: nn.Sequential(nn.Linear(N**2, 256), nn.LeakyReLU(), nn.Linear(256, 256), nn.LeakyReLU(), nn.Linear(256, N**2)) # net t\n",
    "\n",
    "first_mask = np.array(np.concatenate((np.ones(round(N**2/2)), np.zeros(round(N**2/2)))))\n",
    "masks_np = np.stack((first_mask, np.flip(first_mask),first_mask,np.flip(first_mask),first_mask, np.flip(first_mask))) \n",
    "masks = torch.from_numpy(masks_np.astype(np.float32))\n",
    "\n",
    "prior = distributions.MultivariateNormal(torch.zeros(N**2), torch.eye(N**2))      # so we have a total of 3 neural blocks (see fig. 1 of boltzmann generators paper)\n",
    "network = net.RealNVP(nets, nett, masks, prior, x0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(network.loss_ml(training_set))\n",
    "# to-do\n",
    "# 1) implement expectation calculations so we can calculate the loss for the neural net  \n",
    "# 2) implement Boltzmann reweighting \n",
    "# 3) update architecture defined above to match that of Noe et al. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_ml' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-15d0d6d1d98a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# we need to set the gradients to zero before starting to do\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/CS-230-Final-Project/notebooks/../project/networks/net.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, batch, w_ml, w_kl, w_rc)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0mw_kl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_rc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mw_ml\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss_ml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw_kl\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss_kl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw_rc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss_rc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_ml' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam([p for p in network.parameters() if p.requires_grad==True], lr=1e-4)\n",
    "for t in range(1000):    \n",
    "    loss = network.loss(training_set)\n",
    "    \n",
    "    optimizer.zero_grad() # we need to set the gradients to zero before starting to do \n",
    "                          # backpropragation because PyTorch accumulates the gradients on \n",
    "                          # subsequent backward passes.\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if t % 500 == 0:\n",
    "        print('iter %s:' % t, 'loss = %.3f' % loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
