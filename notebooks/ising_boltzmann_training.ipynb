{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To-do\n",
    "1. implement Boltzmann reweighting so expectation is properly calculated \n",
    "2. implement training by energy and KL loss \n",
    "3. (minor) update architecture defined above to match that of Noe et al. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "import torch\n",
    "from torch import distributions\n",
    "from torch import nn\n",
    "from sklearn import datasets\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project.models.ising import IsingModel\n",
    "import project.networks.net as net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Ising Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 90 µs, sys: 0 ns, total: 90 µs\n",
      "Wall time: 93.9 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h = 0.0\n",
    "J = 0.8\n",
    "T = 1.0\n",
    "N = 8\n",
    "\n",
    "ising = IsingModel(h = h, J = J)\n",
    "x0 = ising.init_coords(N)\n",
    "\n",
    "ising.draw_config(x0)\n",
    "%time ising.energy(x0) # energy of a given configuration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training set\n",
    "We'd like to have a $m$ x $N$ matrix containing our training data where $m$ is the number of realizations of the system and $N$ is the number of features (i.e. the flattened dimensions of the system). For example, a training set with 1000 samples of the Ising Model for $N=8$ would be of size (1000, 64). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 2 # as done in Noe et al. for the biwell potential \n",
    "flattened_size = N**2\n",
    "\n",
    "training_set = np.zeros((num_samples,flattened_size), dtype=np.float32)\n",
    "for i in range(num_samples):\n",
    "    training_set[i,:] = ising.init_coords(N).flatten() # generate random configuration \n",
    "training_set = torch.from_numpy(training_set) # convert to PyTorch tensor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an aside, note that although we flatten our configurations for training, the flattening procedure can easily be reversed via the `rehsape()` function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1,  1,  1,  1, -1,  1, -1],\n",
       "       [-1, -1, -1, -1, -1, -1,  1, -1],\n",
       "       [-1,  1,  1, -1,  1, -1, -1,  1],\n",
       "       [-1,  1,  1, -1, -1,  1, -1, -1],\n",
       "       [ 1,  1,  1,  1,  1, -1, -1,  1],\n",
       "       [ 1,  1,  1, -1,  1,  1,  1, -1],\n",
       "       [-1,  1, -1, -1, -1,  1,  1, -1],\n",
       "       [ 1, -1, -1, -1, -1,  1, -1,  1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1,  1,  1,  1, -1,  1, -1],\n",
       "       [-1, -1, -1, -1, -1, -1,  1, -1],\n",
       "       [-1,  1,  1, -1,  1, -1, -1,  1],\n",
       "       [-1,  1,  1, -1, -1,  1, -1, -1],\n",
       "       [ 1,  1,  1,  1,  1, -1, -1,  1],\n",
       "       [ 1,  1,  1, -1,  1,  1,  1, -1],\n",
       "       [-1,  1, -1, -1, -1,  1,  1, -1],\n",
       "       [ 1, -1, -1, -1, -1,  1, -1,  1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x0).flatten().reshape((N,N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boltzmann generator\n",
    "## Define network architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = lambda: nn.Sequential(nn.Linear(N**2, 256), nn.LeakyReLU(), nn.Linear(256, 256), nn.LeakyReLU(), nn.Linear(256, N**2), nn.Tanh()) # net s\n",
    "nett = lambda: nn.Sequential(nn.Linear(N**2, 256), nn.LeakyReLU(), nn.Linear(256, 256), nn.LeakyReLU(), nn.Linear(256, N**2)) # net t\n",
    "\n",
    "first_mask = np.array(np.concatenate((np.ones(round(N**2/2)), np.zeros(round(N**2/2)))))\n",
    "masks_np = np.stack((first_mask, np.flip(first_mask),first_mask,np.flip(first_mask),first_mask, np.flip(first_mask))) \n",
    "masks = torch.from_numpy(masks_np.astype(np.float32))\n",
    "\n",
    "prior = distributions.MultivariateNormal(torch.zeros(N**2), torch.eye(N**2))      # so we have a total of 3 neural blocks (see fig. 1 of boltzmann generators paper)\n",
    "network = net.RealNVP(nets, nett, masks, prior, x0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: loss = 9.036\n",
      "iter 500: loss = -186.242\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam([p for p in network.parameters() if p.requires_grad==True], lr=1e-4)\n",
    "for t in range(1000):    \n",
    "    loss = network.my_loss(batch = training_set)\n",
    "    \n",
    "    optimizer.zero_grad() # we need to set the gradients to zero before starting to do \n",
    "                          # backpropragation because PyTorch accumulates the gradients on \n",
    "                          # subsequent backward passes.\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if t % 100 == 0:\n",
    "        print('iter %s:' % t, 'loss = %.3f' % loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
