{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To-do\n",
    "1. implement Boltzmann reweighting so expectation is properly calculated \n",
    "2. implement training by energy and KL loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "import torch\n",
    "from torch import distributions\n",
    "from torch import nn\n",
    "from sklearn import datasets\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project.models.ising import IsingModel\n",
    "import project.networks.net as net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Ising Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 94 µs, sys: 0 ns, total: 94 µs\n",
      "Wall time: 97 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-3.2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h = 0.0\n",
    "J = 0.8\n",
    "T = 1.0\n",
    "N = 8\n",
    "\n",
    "ising = IsingModel(h = h, J = J)\n",
    "x0 = ising.init_coords(N)\n",
    "\n",
    "ising.draw_config(x0)\n",
    "%time ising.energy(x0) # energy of a given configuration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training set\n",
    "We'd like to have a $m$ x $N$ matrix containing our training data where $m$ is the number of realizations of the system and $N$ is the number of features (i.e. the flattened dimensions of the system). For example, a training set with 1000 samples of the Ising Model for $N=8$ would be of size (1000, 64). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 2 # as done in Noe et al. for the biwell potential \n",
    "flattened_size = N**2\n",
    "\n",
    "training_set = np.zeros((num_samples,flattened_size), dtype=np.float32)\n",
    "for i in range(num_samples):\n",
    "    training_set[i,:] = ising.init_coords(N).flatten() # generate random configuration \n",
    "training_set = torch.from_numpy(training_set) # convert to PyTorch tensor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an aside, note that although we flatten our configurations for training, the flattening procedure can easily be reversed via the `rehsape()` function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1, -1,  1,  1, -1, -1,  1],\n",
       "       [-1, -1, -1,  1, -1,  1,  1,  1],\n",
       "       [ 1, -1,  1, -1,  1, -1,  1,  1],\n",
       "       [-1, -1,  1, -1,  1, -1,  1,  1],\n",
       "       [-1, -1, -1,  1,  1,  1, -1, -1],\n",
       "       [ 1,  1,  1, -1,  1,  1,  1,  1],\n",
       "       [-1,  1,  1, -1,  1,  1, -1,  1],\n",
       "       [ 1, -1,  1,  1,  1,  1,  1,  1]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1, -1,  1,  1, -1, -1,  1],\n",
       "       [-1, -1, -1,  1, -1,  1,  1,  1],\n",
       "       [ 1, -1,  1, -1,  1, -1,  1,  1],\n",
       "       [-1, -1,  1, -1,  1, -1,  1,  1],\n",
       "       [-1, -1, -1,  1,  1,  1, -1, -1],\n",
       "       [ 1,  1,  1, -1,  1,  1,  1,  1],\n",
       "       [-1,  1,  1, -1,  1,  1, -1,  1],\n",
       "       [ 1, -1,  1,  1,  1,  1,  1,  1]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x0).flatten().reshape((N,N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boltzmann generator\n",
    "## Define network architecture \n",
    "In the following we use 3 hidden layers for the translation and scaling networks and a total of four stacked RealNVP blocks (as defined by `masks`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 100 # number of hidden layers\n",
    "nets = lambda: nn.Sequential(nn.Linear(N**2, n_hidden), nn.Tanh(), nn.Linear(n_hidden, n_hidden), nn.Tanh(), nn.Linear(n_hidden, N**2), nn.Tanh()) # net s\n",
    "nett = lambda: nn.Sequential(nn.Linear(N**2, n_hidden), nn.ReLU(), nn.Linear(n_hidden, n_hidden), nn.ReLU(), nn.Linear(n_hidden, N**2)) # net t\n",
    "\n",
    "first_mask = np.array(np.concatenate((np.ones(round(N**2/2)), np.zeros(round(N**2/2)))))\n",
    "masks_np = np.stack((first_mask, np.flip(first_mask),first_mask,np.flip(first_mask),first_mask, np.flip(first_mask),first_mask, np.flip(first_mask))) \n",
    "masks = torch.from_numpy(masks_np.astype(np.float32))\n",
    "\n",
    "prior = distributions.MultivariateNormal(torch.zeros(N**2), torch.eye(N**2))      # so we have a total of 3 neural blocks (see fig. 1 of boltzmann generators paper)\n",
    "network = net.RealNVP(nets, nett, masks, prior, x0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model \n",
    "### Train by example first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: loss = 10.780\n",
      "iter 100: loss = -139.640\n",
      "iter 200: loss = -218.778\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam([p for p in network.parameters() if p.requires_grad==True], lr=1e-4)\n",
    "for t in range(300):    \n",
    "    loss = network.my_loss(batch = training_set, w_ml = 1.0, w_kl = 0.0)\n",
    "    \n",
    "    optimizer.zero_grad() # we need to set the gradients to zero before starting to do \n",
    "                          # backpropragation because PyTorch accumulates the gradients on \n",
    "                          # subsequent backward passes.\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if t % 100 == 0:\n",
    "        print('iter %s:' % t, 'loss = %.3f' % loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next train by both example and by energy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([p for p in network.parameters() if p.requires_grad==True], lr=1e-4)\n",
    "for t in range(500):    \n",
    "    loss = network.my_loss(batch = training_set, w_ml = 1.0, w_kl = 1.0)\n",
    "    \n",
    "    optimizer.zero_grad() # we need to set the gradients to zero before starting to do \n",
    "                          # backpropragation because PyTorch accumulates the gradients on \n",
    "                          # subsequent backward passes.\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if t % 100 == 0:\n",
    "        print('iter %s:' % t, 'loss = %.3f' % loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
