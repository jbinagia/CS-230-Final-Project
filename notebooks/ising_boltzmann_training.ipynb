{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To-do \n",
    "- Debug Metropolis Monte Carlo step during training by energy so we can sample low-energy states efficiently. \n",
    "- Generally optimize methods in net.py (several redundant calls) \n",
    "- (quick and easy) generalize energy functions (they need to expect PyTorch tensors instead of numpy arrays). So for example every `np.sum()` should be `torch.sum()`, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "import torch\n",
    "from torch import distributions\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from sklearn import datasets\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2 # Reload all modules every time before executing the Python code typed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Ising Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 336 µs, sys: 0 ns, total: 336 µs\n",
      "Wall time: 339 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0625"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAAFbCAYAAADiN/RYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAB1BJREFUeJzt2zFuY9cZhuH/ejjIxGMjRcZFVuDC\nKQSEK9AC3KR0NwX3ZBXyTrQCD6AiG0jvVCkCxMVJ4aQIEIcaSBy+lJ6nI3CKD5fii4sDaFtrDQDn\n99m5BwDwC0EGiBBkgAhBBogQZIAIQQaIEGSACEEGiBBkgIjdsQPbth1m5vDLh92ftje/O/WmF+H3\n//j7uSc8K3/77ZfnnnDUpXznl/AsZy7nec7M/DT//Gmt9dWxc9vH/Ov0Z5+/W7uvv33UMH7x/v7u\n3BOeldur63NPOOpSvvNLeJYzl/M8Z2a+n79+WGvtj51zZQEQIcgAEYIMECHIABGCDBAhyAARggwQ\nIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIMECHIABGCDBAh\nyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIMELE7\ndmDbtsPMHGZmvphX89393clHPcbt1fW5J8D/5G+TY46+Ia+1btZa+7XW/s28+hSbAF4kVxYAEYIM\nECHIABGCDBAhyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQ\nIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIMECHIABGCDBAh\nyAARggwQIcgAEYIMECHIABG7Ywe2bTvMzGFmZl6/ndtvrk+96VHe39+de8KD3F61n+N/XMrzvISd\nl/Kdcz5H35DXWjdrrf1aa7/t3nyKTQAvkisLgAhBBogQZIAIQQaIEGSACEEGiBBkgAhBBogQZIAI\nQQaIEGSACEEGiBBkgAhBBogQZIAIQQaIEGSACEEGiBBkgAhBBogQZIAIQQaIEGSACEEGiBBkgAhB\nBogQZIAIQQaIEGSACEEGiBBkgAhBBogQZIAIQQaIEGSACEEGiBBkgAhBBojYHTuwbdthZg4zM/P6\n7an3ALxY21rrwYe/2n6z/jx/OOEc4Nxur67PPeHZ+fn+hw9rrf2xc64sACIEGSBCkAEiBBkgQpAB\nIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEi\nBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIE\nGSBCkAEidscObNt2mJnDzMwX8+rkgwBeqm2t9eDDn33+bu2+/vaEc16O9/d3557wrNxeXZ97Avyq\nn+9/+LDW2h8758oCIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQ\nASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpAB\nIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBid+zAtm2HmTnMzMzrt6fe82jv7+/OPeFB\nbq+uzz3hQS7leV7KzktwKX+bz9HRN+S11s1aa7/W2m+7N59iE8CL5MoCIEKQASIEGSBCkAEiBBkg\nQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBC\nkAEiBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQ\nASIEGSBid+zAtm2HmTnMzHwxr+a7+7uTj4KPdXt1fe4JR72/kN/Opey8JN8/8NzRN+S11s1aa7/W\n2r+ZV4+cBcCvcWUBECHIABGCDBAhyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIMECHI\nABGCDBAhyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQIcgA\nEYIMECHIABGCDBAhyAARggwQIcgAEYIMECHIABGCDBCxO3Zg27bDzBxmZub127n95vrUmx7l/f3d\nuSc8yKXs5OncXrV/O5fmOf6Gjr4hr7Vu1lr7tdZ+2735FJsAXiRXFgARggwQIcgAEYIMECHIABGC\nDBAhyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIM\nECHIABGCDBAhyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQ\nsa21/v+BbTvMzOHfH/84M3859agn8G5mfjr3iCMuYeOMnU/Nzqd1KTu/Xmt9eezQ0SD/1+Ft+3Gt\ntX/UrE/gEnZewsYZO5+anU/rue10ZQEQIcgAER8b5JuTrHh6l7DzEjbO2PnU7Hxaz2rnR90hA3A6\nriwAIgQZIEKQASIEGSBCkAEi/gUx+tuW0yYOJgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f926b05a6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h = 0.0\n",
    "J = 0.8\n",
    "T = 1.0\n",
    "N = 8\n",
    "\n",
    "ising = IsingModel(h = h, J = J)\n",
    "x0 = ising.init_coords(N)\n",
    "ising.draw_config(x0)\n",
    "\n",
    "%time ising.energy(torch.from_numpy(x0)) # energy of a given configuration \n",
    "ising.oprm(x0) # calculate average magnetization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training set\n",
    "We'd like to have a $m$ x $N$ matrix containing our training data where $m$ is the number of realizations of the system and $N$ is the number of features (i.e. the flattened dimensions of the system). For example, a training set with 1000 samples of the Ising Model for $N=8$ would be of size (1000, 64). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_samples = 12800 # Use 1000 at run-time as done in Noe et al. for the biwell potential \n",
    "flattened_size = N**2\n",
    "\n",
    "training_set = np.zeros((num_samples,flattened_size), dtype=np.float32)\n",
    "for i in range(num_samples):\n",
    "    training_set[i,:] = ising.init_coords(N).flatten() # generate random configuration \n",
    "training_set = torch.from_numpy(training_set) # convert to PyTorch tensor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an aside, note that although we flatten our configurations for training, the flattening procedure can easily be reversed via the `rehsape()` function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1, -1,  1, -1,  1,  1,  1],\n",
       "       [ 1, -1, -1,  1, -1,  1, -1,  1],\n",
       "       [ 1, -1,  1,  1, -1, -1,  1, -1],\n",
       "       [-1,  1, -1, -1, -1, -1, -1,  1],\n",
       "       [-1, -1, -1, -1, -1,  1,  1,  1],\n",
       "       [-1,  1,  1,  1, -1,  1,  1,  1],\n",
       "       [ 1, -1,  1, -1,  1, -1, -1, -1],\n",
       "       [-1, -1, -1, -1,  1, -1, -1, -1]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1, -1,  1, -1,  1,  1,  1],\n",
       "       [ 1, -1, -1,  1, -1,  1, -1,  1],\n",
       "       [ 1, -1,  1,  1, -1, -1,  1, -1],\n",
       "       [-1,  1, -1, -1, -1, -1, -1,  1],\n",
       "       [-1, -1, -1, -1, -1,  1,  1,  1],\n",
       "       [-1,  1,  1,  1, -1,  1,  1,  1],\n",
       "       [ 1, -1,  1, -1,  1, -1, -1, -1],\n",
       "       [-1, -1, -1, -1,  1, -1, -1, -1]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x0).flatten().reshape((N,N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram energies of training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously I was calculating energy as: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f92894096a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(ising.energy_vec(training_set))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is clearly different from: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f926ad97358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "energies = []\n",
    "for i in range(training_set.shape[0]):\n",
    "    energies.append(ising.energy(training_set[i,:].reshape((N,N))))\n",
    "plt.hist(energies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram Boltzmann weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD8CAYAAAB6paOMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAEShJREFUeJzt3X+s3XV9x/HnSxBd1EmVoqQFymYz\nxcQfpAKby0RY+LmsOCErcdqxJk0cM7poZt0fsoFk+M8wLMOFSWM1m5WhDiI41xSIWRSlyA9FVKoy\n6EpstYgzTmfxvT/Op+4A9/aee9t7zrGf5yO5Od/v+/s55/v+ntueV78/zrepKiRJ/XrGpBuQJE2W\nQSBJnTMIJKlzBoEkdc4gkKTOGQSS1DmDQJI6ZxBIUucMAknq3OGTbmB/jjrqqFqxYsWk25CkXyp3\n3XXX96pq6ajjpzoIVqxYwbZt2ybdhiT9Uknyn/MZ76EhSeqcQSBJnTMIJKlzBoEkdc4gkKTOjRQE\nSR5K8pUk9yTZ1movSLIlyYPtcUmrJ8nVSbYnuS/JSUOvs7aNfzDJ2sXZJEnSfMxnj+D1VfWqqlrV\n5jcAW6tqJbC1zQOcA6xsP+uBD8IgOIBLgVOAk4FL94WHJGlyDuTQ0GpgU5veBJw/VP9IDdwBHJnk\nGOAsYEtV7amqx4AtwNkHsH5J0kEwahAU8O9J7kqyvtVeVFWPArTHo1t9GfDI0HN3tNpsdUnSBI36\nzeLXVtXOJEcDW5J8fT9jM0Ot9lN/8pMHQbMe4LjjjhuxPfVqxYabJ7buh648b2Lrlg6mkfYIqmpn\ne9wFfIrBMf7vtkM+tMddbfgO4Nihpy8Hdu6n/tR1XVtVq6pq1dKlI98qQ5K0QHMGQZLnJHnevmng\nTOCrwE3Avit/1gI3tumbgLe0q4dOBR5vh44+C5yZZEk7SXxmq0mSJmiUQ0MvAj6VZN/4f66qf0ty\nJ3B9knXAw8CFbfwtwLnAduDHwMUAVbUnyeXAnW3cZVW156BtiSRpQeYMgqr6NvDKGerfB86YoV7A\nJbO81kZg4/zblCQtFr9ZLEmdMwgkqXMGgSR1ziCQpM4ZBJLUOYNAkjpnEEhS5wwCSeqcQSBJnTMI\nJKlzBoEkdc4gkKTOGQSS1DmDQJI6ZxBIUucMAknqnEEgSZ0zCCSpcwaBJHXOIJCkzhkEktQ5g0CS\nOmcQSFLnDAJJ6pxBIEmdMwgkqXMGgSR1ziCQpM4ZBJLUOYNAkjpnEEhS5wwCSercyEGQ5LAkdyf5\ndJs/IckXkzyY5ONJjmj1Z7X57W35iqHXeE+rfyPJWQd7YyRJ8zefPYK3Aw8Mzb8fuKqqVgKPAeta\nfR3wWFW9BLiqjSPJicAa4OXA2cA1SQ47sPYlSQdqpCBIshw4D/hQmw9wOnBDG7IJOL9Nr27ztOVn\ntPGrgc1V9dOq+g6wHTj5YGyEJGnhRt0j+ADwF8DP2/wLgR9U1d42vwNY1qaXAY8AtOWPt/G/qM/w\nHEnShMwZBEl+D9hVVXcNl2cYWnMs299zhte3Psm2JNt27949V3uSpAM0yh7Ba4HfT/IQsJnBIaEP\nAEcmObyNWQ7sbNM7gGMB2vLnA3uG6zM85xeq6tqqWlVVq5YuXTrvDZIkzc+cQVBV76mq5VW1gsHJ\n3lur6k3AbcAFbdha4MY2fVObpy2/taqq1de0q4pOAFYCXzpoWyJJWpDD5x4yq3cDm5O8D7gbuK7V\nrwM+mmQ7gz2BNQBVdX+S64GvAXuBS6rqiQNYvyTpIJhXEFTV7cDtbfrbzHDVT1X9BLhwludfAVwx\n3yYlSYvHbxZLUucMAknqnEEgSZ0zCCSpcwaBJHXOIJCkzhkEktQ5g0CSOmcQSFLnDAJJ6pxBIEmd\nMwgkqXMGgSR1ziCQpM4ZBJLUOYNAkjpnEEhS5wwCSeqcQSBJnTMIJKlzBoEkdc4gkKTOGQSS1DmD\nQJI6ZxBIUucMAknqnEEgSZ0zCCSpcwaBJHXOIJCkzhkEktQ5g0CSOnf4pBvQoWHFhpsn3YKkBZpz\njyDJs5N8Kcm9Se5P8tetfkKSLyZ5MMnHkxzR6s9q89vb8hVDr/WeVv9GkrMWa6MkSaMb5dDQT4HT\nq+qVwKuAs5OcCrwfuKqqVgKPAeva+HXAY1X1EuCqNo4kJwJrgJcDZwPXJDnsYG6MJGn+5gyCGvhR\nm31m+yngdOCGVt8EnN+mV7d52vIzkqTVN1fVT6vqO8B24OSDshWSpAUb6WRxksOS3APsArYA3wJ+\nUFV725AdwLI2vQx4BKAtfxx44XB9hudIkiZkpCCoqieq6lXAcgb/in/ZTMPaY2ZZNlv9SZKsT7It\nybbdu3eP0p4k6QDM6/LRqvoBcDtwKnBkkn1XHS0HdrbpHcCxAG3584E9w/UZnjO8jmuralVVrVq6\ndOl82pMkLcAoVw0tTXJkm/4V4HeBB4DbgAvasLXAjW36pjZPW35rVVWrr2lXFZ0ArAS+dLA2RJK0\nMKN8j+AYYFO7wucZwPVV9ekkXwM2J3kfcDdwXRt/HfDRJNsZ7AmsAaiq+5NcD3wN2AtcUlVPHNzN\nkSTN15xBUFX3Aa+eof5tZrjqp6p+Alw4y2tdAVwx/zYlSYvFW0xIUucMAknqnEEgSZ0zCCSpcwaB\nJHXOIJCkzhkEktQ5g0CSOmcQSFLnDAJJ6pxBIEmdMwgkqXMGgSR1ziCQpM4ZBJLUOYNAkjpnEEhS\n5wwCSeqcQSBJnTMIJKlzBoEkdc4gkKTOGQSS1DmDQJI6ZxBIUucMAknqnEEgSZ0zCCSpcwaBJHXO\nIJCkzhkEktQ5g0CSOjdnECQ5NsltSR5Icn+St7f6C5JsSfJge1zS6klydZLtSe5LctLQa61t4x9M\nsnbxNkuSNKpR9gj2Au+sqpcBpwKXJDkR2ABsraqVwNY2D3AOsLL9rAc+CIPgAC4FTgFOBi7dFx6S\npMmZMwiq6tGq+nKb/m/gAWAZsBrY1IZtAs5v06uBj9TAHcCRSY4BzgK2VNWeqnoM2AKcfVC3RpI0\nb/M6R5BkBfBq4IvAi6rqURiEBXB0G7YMeGToaTtabba6JGmCDh91YJLnAp8A3lFVP0wy69AZarWf\n+lPXs57BISWOO+64UdsTsGLDzZNuQdIvoZH2CJI8k0EI/FNVfbKVv9sO+dAed7X6DuDYoacvB3bu\np/4kVXVtVa2qqlVLly6dz7ZIkhZglKuGAlwHPFBVfzu06CZg35U/a4Ebh+pvaVcPnQo83g4dfRY4\nM8mSdpL4zFaTJE3QKIeGXgu8GfhKknta7S+BK4Hrk6wDHgYubMtuAc4FtgM/Bi4GqKo9SS4H7mzj\nLquqPQdlKyRJCzZnEFTVfzDz8X2AM2YYX8Als7zWRmDjfBqUJC0uv1ksSZ0zCCSpcwaBJHXOIJCk\nzhkEktQ5g0CSOmcQSFLnDAJJ6pxBIEmdMwgkqXMj34Za0pNN6rbfD1153kTWq0OXewSS1DmDQJI6\nZxBIUucMAknqnEEgSZ0zCCSpcwaBJHXOIJCkzhkEktQ5g0CSOmcQSFLnDAJJ6pxBIEmdMwgkqXMG\ngSR1ziCQpM4ZBJLUOYNAkjpnEEhS5wwCSeqcQSBJnTMIJKlzcwZBko1JdiX56lDtBUm2JHmwPS5p\n9SS5Osn2JPclOWnoOWvb+AeTrF2czZEkzdcoewQfBs5+Sm0DsLWqVgJb2zzAOcDK9rMe+CAMggO4\nFDgFOBm4dF94SJIma84gqKrPAXueUl4NbGrTm4Dzh+ofqYE7gCOTHAOcBWypqj1V9RiwhaeHiyRp\nAhZ6juBFVfUoQHs8utWXAY8MjdvRarPVnybJ+iTbkmzbvXv3AtuTJI3qYJ8szgy12k/96cWqa6tq\nVVWtWrp06UFtTpL0dAsNgu+2Qz60x12tvgM4dmjccmDnfuqSpAlbaBDcBOy78mctcONQ/S3t6qFT\ngcfboaPPAmcmWdJOEp/ZapKkCTt8rgFJPgacBhyVZAeDq3+uBK5Psg54GLiwDb8FOBfYDvwYuBig\nqvYkuRy4s427rKqeegJakjQBcwZBVV00y6IzZhhbwCWzvM5GYOO8upMkLTq/WSxJnTMIJKlzBoEk\ndc4gkKTOGQSS1DmDQJI6ZxBIUucMAknqnEEgSZ0zCCSpcwaBJHXOIJCkzhkEktS5Oe8+Kmm6rNhw\n88TW/dCV501s3Vo87hFIUucMAknqnEEgSZ0zCCSpcwaBJHXOIJCkzhkEktQ5g0CSOmcQSFLn/Gbx\nIpjkNz8lab7cI5CkzhkEktQ5g0CSOmcQSFLnDAJJ6pxBIEmd8/JRSSOb1KXR/oc4i8s9Aknq3NiD\nIMnZSb6RZHuSDeNevyTpycYaBEkOA/4eOAc4EbgoyYnj7EGS9GTj3iM4GdheVd+uqv8FNgOrx9yD\nJGnIuE8WLwMeGZrfAZyyWCvznj/SocGT1Itr3EGQGWr1pAHJemB9m/1Rkm8sUi9HAd9bpNc+UNPa\n27T2Bfa2ENPaF0xJb3n/jOWp6G0Gw30dP58njjsIdgDHDs0vB3YOD6iqa4FrF7uRJNuqatVir2ch\nprW3ae0L7G0hprUvsLeFOJC+xn2O4E5gZZITkhwBrAFuGnMPkqQhY90jqKq9Sf4M+CxwGLCxqu4f\nZw+SpCcb+zeLq+oW4JZxr3cGi3746QBMa2/T2hfY20JMa19gbwux4L5SVXOPkiQdsrzFhCR17pAP\ngrluaZHkqiT3tJ9vJvnBlPR1XJLbktyd5L4k546jrxF7Oz7J1tbX7UmWj6mvjUl2JfnqLMuT5OrW\n931JThpHXyP29tIkX0jy0yTvmqK+3tTeq/uSfD7JK6eot9Wtr3uSbEvy29PS29C41yR5IskF09BX\nktOSPD70mfbekV64qg7ZHwYnpL8F/BpwBHAvcOJ+xr+NwQnsiffF4HjfW9v0icBD0/KeAf8CrG3T\npwMfHVNvvwOcBHx1luXnAp9h8H2VU4EvjvHP2ly9HQ28BrgCeNcU9fVbwJI2fc6UvWfP5f8PX78C\n+Pq09NbGHAbcyuCc5wXT0BdwGvDp+b7uob5HMN9bWlwEfGxK+irgV9v083nK9y0m3NuJwNY2fdsM\nyxdFVX0O2LOfIauBj9TAHcCRSY6Zht6qaldV3Qn8bBz9DK13rr4+X1WPtdk7GHy3ZyxG6O1H1T7d\ngOfwlC+fLqYR/qzB4B+OnwB2LX5HAyP2NW+HehDMdEuLZTMNTHI8cAKDhJ+Gvv4K+KMkOxj8i+Nt\nY+gLRuvtXuCNbfoNwPOSvHAMvc1l5N+3ZrSOwR7V1EjyhiRfB24G/mTS/eyTZBmDP/v/MOleZvCb\nSe5N8pkkLx/lCYd6EMx5S4sha4AbquqJRexnn1H6ugj4cFUtZ3DI46NJxvH7GqW3dwGvS3I38Drg\nv4C9i93YCObz+9aQJK9nEATvnnQvw6rqU1X1UuB84PJJ9zPkA8C7x/R5MR9fBo6vqlcCfwf86yhP\nOtT/h7I5b2kxZA1wyaJ3NDBKX+uAswGq6gtJns3gXiKLvRs6ym1AdgJ/AJDkucAbq+rxRe5rFPP5\nfatJ8grgQ8A5VfX9Sfczk6r6XJJfT3JUVU3DfX5WAZuTwODv5blJ9lbVSB+8i6Wqfjg0fUuSa0Z5\nzw71PYKRbmmR5DeAJcAXpqivh4EzWn8vA54N7J6G3pIcNbR38h5g4xj6GsVNwFva1UOnAo9X1aOT\nbmqaJTkO+CTw5qr65qT7GZbkJWmftO0KsCOAqQiqqjqhqlZU1QrgBuBPJx0CAElePPSenczgM37O\n9+yQ3iOoWW5pkeQyYFtV7fuAuwjYPHRiahr6eifwj0n+nMHhjT8eR38j9nYa8DdJCvgcY9qTSvKx\ntu6j2rmTS4Fntr7/gcG5lHOB7cCPgYvH0dcovSV5MbCNwQUAP0/yDgZXY/1wlpccS1/Ae4EXAte0\nz4+9NaYbqo3Q2xsZBPvPgP8B/nBcf0dH6G0iRujrAuCtSfYyeM/WjPKe+c1iSercoX5oSJI0B4NA\nkjpnEEhS5wwCSeqcQSBJnTMIJKlzBoEkdc4gkKTO/R9F28D9TJxBjwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f926b06d400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# KH I added a temperature so the weights could be visualized\n",
    "weights = np.exp(-np.array(energies)/100)\n",
    "plt.hist(weights)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boltzmann generator\n",
    "## Define network architecture \n",
    "In the following we use 3 hidden layers for the translation and scaling networks and a total of four stacked RealNVP blocks (as defined by `masks`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden = 100 # number of hidden layers\n",
    "nets = lambda: nn.Sequential(nn.Linear(N**2, n_hidden), nn.Tanh(), nn.Linear(n_hidden, n_hidden), nn.Tanh(), nn.Linear(n_hidden, N**2), nn.Tanh()) # net s\n",
    "nett = lambda: nn.Sequential(nn.Linear(N**2, n_hidden), nn.ReLU(), nn.Linear(n_hidden, n_hidden), nn.ReLU(), nn.Linear(n_hidden, N**2)) # net t\n",
    "\n",
    "first_mask = np.array(np.concatenate((np.ones(round(N**2/2)), np.zeros(round(N**2/2)))))\n",
    "masks_np = np.stack(\n",
    "                    ( first_mask, np.flip(first_mask, axis=0),\n",
    "                      first_mask, np.flip(first_mask, axis=0),\n",
    "                      first_mask, np.flip(first_mask, axis=0),\n",
    "                      first_mask, np.flip(first_mask, axis=0) )\n",
    "                   ) \n",
    "masks = torch.from_numpy(masks_np.astype(np.float32))\n",
    "\n",
    "prior = distributions.MultivariateNormal(torch.zeros(N**2), torch.eye(N**2))      # so we have a total of 3 neural blocks (see fig. 1 of boltzmann generators paper)\n",
    "network = net.RealNVP(nets, nett, masks, prior, ising, x0.shape)\n",
    "\n",
    "# network.system.energy(network.g(training_set)) # energy functions need to be rewritten with PyTorch tensors in mind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $\\log R_{xz} = - \\log R_{zx}$, as demonstrated below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1162], grad_fn=<AddBackward0>)\n",
      "tensor([-1.1162], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z, log_Rxz = network.f(training_set[0:1,:])\n",
    "x, log_Rzx = network.g(z)\n",
    "print(log_Rzx)\n",
    "print(log_Rxz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Energies can not be calculated via the `calculate_energy()` method of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f92894d6d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(network.calculate_energy(training_set))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the average energy using the Boltzmann weights as: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-38.3434)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.expected_value(network.calculate_energy(training_set), training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model \n",
    "The training schedule accordingly the main text of Noe et al. is as follows: \n",
    "- Begin with a buffer of 10,000 real space configurations \n",
    "- Train by example, minimizing $J_{ML}$, using a batch size of 128 for 20 iterations (JB: so not even one full epoch?)\n",
    "- Switch to batch size of 1000 and for each batch (iteration) we now perform a Metropolis Monte Carlo step in latent space for each sample in the given batch. \n",
    "\n",
    "### Train by example first\n",
    "Minimizing $J_{ML}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 100: loss = -7.019\n",
      "iter 200: loss = -28.649\n",
      "iter 300: loss = -32.306\n",
      "iter 400: loss = -33.711\n",
      "iter 500: loss = -34.739\n",
      "iter 600: loss = -35.531\n",
      "iter 700: loss = -36.169\n",
      "iter 800: loss = -36.741\n",
      "iter 900: loss = -37.226\n",
      "iter 1000: loss = -37.749\n",
      "iter 1100: loss = -38.240\n",
      "iter 1200: loss = -38.728\n",
      "iter 1300: loss = -39.190\n",
      "iter 1400: loss = -39.712\n",
      "iter 1500: loss = -39.988\n",
      "iter 1600: loss = -40.998\n",
      "iter 1700: loss = -41.413\n",
      "iter 1800: loss = -42.038\n",
      "iter 1900: loss = -42.704\n",
      "iter 2000: loss = -43.114\n",
      "iter 2100: loss = -44.401\n",
      "iter 2200: loss = -44.308\n",
      "iter 2300: loss = -44.816\n",
      "iter 2400: loss = -45.664\n",
      "iter 2500: loss = -46.279\n",
      "iter 2600: loss = -47.310\n",
      "iter 2700: loss = -49.106\n",
      "iter 2800: loss = -49.383\n",
      "iter 2900: loss = -50.517\n",
      "iter 3000: loss = -49.875\n",
      "iter 3100: loss = -51.652\n",
      "iter 3200: loss = -51.477\n",
      "iter 3300: loss = -53.279\n",
      "iter 3400: loss = -54.120\n",
      "iter 3500: loss = -55.063\n",
      "iter 3600: loss = -56.216\n",
      "iter 3700: loss = -55.933\n",
      "iter 3800: loss = -56.397\n",
      "iter 3900: loss = -57.083\n",
      "iter 4000: loss = -58.287\n",
      "iter 4100: loss = -58.580\n",
      "iter 4200: loss = -60.251\n",
      "iter 4300: loss = -60.877\n",
      "iter 4400: loss = -59.522\n",
      "iter 4500: loss = -63.398\n",
      "iter 4600: loss = -63.532\n",
      "iter 4700: loss = -64.823\n",
      "iter 4800: loss = -65.292\n",
      "iter 4900: loss = -64.566\n",
      "iter 5000: loss = -65.585\n",
      "iter 5100: loss = -66.031\n",
      "iter 5200: loss = -68.371\n",
      "iter 5300: loss = -68.225\n",
      "iter 5400: loss = -67.270\n",
      "iter 5500: loss = -65.654\n",
      "iter 5600: loss = -68.541\n",
      "iter 5700: loss = -69.440\n",
      "iter 5800: loss = -71.019\n",
      "iter 5900: loss = -69.476\n",
      "iter 6000: loss = -72.094\n",
      "iter 6100: loss = -70.958\n",
      "iter 6200: loss = -72.246\n",
      "iter 6300: loss = -72.572\n",
      "iter 6400: loss = -72.594\n",
      "iter 6500: loss = -73.235\n",
      "iter 6600: loss = -71.508\n",
      "iter 6700: loss = -73.094\n",
      "iter 6800: loss = -71.457\n",
      "iter 6900: loss = -73.833\n",
      "iter 7000: loss = -74.910\n",
      "iter 7100: loss = -72.351\n",
      "iter 7200: loss = -74.066\n",
      "iter 7300: loss = -73.483\n",
      "iter 7400: loss = -76.081\n",
      "iter 7500: loss = -75.425\n",
      "iter 7600: loss = -72.991\n",
      "iter 7700: loss = -74.988\n",
      "iter 7800: loss = -75.225\n",
      "iter 7900: loss = -73.244\n",
      "iter 8000: loss = -75.131\n",
      "iter 8100: loss = -74.530\n",
      "iter 8200: loss = -74.485\n",
      "iter 8300: loss = -75.833\n",
      "iter 8400: loss = -78.271\n",
      "iter 8500: loss = -78.058\n",
      "iter 8600: loss = -75.021\n",
      "iter 8700: loss = -77.859\n",
      "iter 8800: loss = -79.066\n",
      "iter 8900: loss = -76.007\n",
      "iter 9000: loss = -78.973\n",
      "iter 9100: loss = -76.882\n",
      "iter 9200: loss = -79.198\n",
      "iter 9300: loss = -78.810\n",
      "iter 9400: loss = -78.456\n",
      "iter 9500: loss = -77.474\n",
      "iter 9600: loss = -79.798\n",
      "iter 9700: loss = -80.884\n",
      "iter 9800: loss = -77.108\n",
      "iter 9900: loss = -74.705\n",
      "iter 10000: loss = -76.921\n",
      "iter 10100: loss = -81.723\n",
      "iter 10200: loss = -80.125\n",
      "iter 10300: loss = -79.790\n",
      "iter 10400: loss = -79.900\n",
      "iter 10500: loss = -80.038\n",
      "iter 10600: loss = -81.607\n",
      "iter 10700: loss = -81.537\n",
      "iter 10800: loss = -77.211\n",
      "iter 10900: loss = -81.779\n",
      "iter 11000: loss = -82.378\n",
      "iter 11100: loss = -80.404\n",
      "iter 11200: loss = -80.706\n",
      "iter 11300: loss = -81.148\n",
      "iter 11400: loss = -82.128\n",
      "iter 11500: loss = -80.667\n",
      "iter 11600: loss = -84.270\n",
      "iter 11700: loss = -81.890\n",
      "iter 11800: loss = -83.253\n",
      "iter 11900: loss = -79.900\n",
      "iter 12000: loss = -79.861\n",
      "iter 12100: loss = -80.235\n",
      "iter 12200: loss = -81.506\n",
      "iter 12300: loss = -83.112\n",
      "iter 12400: loss = -82.650\n",
      "iter 12500: loss = -79.582\n",
      "iter 12600: loss = -81.649\n",
      "iter 12700: loss = -85.060\n",
      "iter 12800: loss = -84.450\n",
      "iter 12900: loss = -82.332\n",
      "iter 13000: loss = -79.607\n",
      "iter 13100: loss = -82.512\n",
      "iter 13200: loss = -79.892\n",
      "iter 13300: loss = -84.848\n",
      "iter 13400: loss = -79.584\n",
      "iter 13500: loss = -84.286\n",
      "iter 13600: loss = -82.124\n",
      "iter 13700: loss = -85.353\n",
      "iter 13800: loss = -83.682\n",
      "iter 13900: loss = -83.780\n",
      "iter 14000: loss = -84.466\n",
      "iter 14100: loss = -84.505\n",
      "iter 14200: loss = -82.744\n",
      "iter 14300: loss = -83.973\n",
      "iter 14400: loss = -85.413\n",
      "iter 14500: loss = -84.859\n",
      "iter 14600: loss = -87.206\n",
      "iter 14700: loss = -85.655\n",
      "iter 14800: loss = -87.248\n",
      "iter 14900: loss = -85.641\n",
      "iter 15000: loss = -87.333\n",
      "iter 15100: loss = -86.918\n",
      "iter 15200: loss = -88.792\n",
      "iter 15300: loss = -84.537\n",
      "iter 15400: loss = -85.636\n",
      "iter 15500: loss = -86.129\n",
      "iter 15600: loss = -87.048\n",
      "iter 15700: loss = -87.220\n",
      "iter 15800: loss = -87.010\n",
      "iter 15900: loss = -88.404\n",
      "iter 16000: loss = -86.387\n",
      "iter 16100: loss = -85.716\n",
      "iter 16200: loss = -82.581\n",
      "iter 16300: loss = -82.582\n",
      "iter 16400: loss = -87.168\n",
      "iter 16500: loss = -87.214\n",
      "iter 16600: loss = -87.087\n",
      "iter 16700: loss = -88.546\n",
      "iter 16800: loss = -89.838\n",
      "iter 16900: loss = -87.182\n",
      "iter 17000: loss = -88.898\n",
      "iter 17100: loss = -86.357\n",
      "iter 17200: loss = -86.679\n",
      "iter 17300: loss = -83.807\n",
      "iter 17400: loss = -89.584\n",
      "iter 17500: loss = -88.478\n",
      "iter 17600: loss = -89.372\n",
      "iter 17700: loss = -88.623\n",
      "iter 17800: loss = -86.184\n",
      "iter 17900: loss = -86.280\n",
      "iter 18000: loss = -87.751\n",
      "iter 18100: loss = -90.434\n",
      "iter 18200: loss = -88.873\n",
      "iter 18300: loss = -87.764\n",
      "iter 18400: loss = -89.195\n",
      "iter 18500: loss = -87.084\n",
      "iter 18600: loss = -90.103\n",
      "iter 18700: loss = -88.054\n",
      "iter 18800: loss = -86.525\n",
      "iter 18900: loss = -90.648\n",
      "iter 19000: loss = -89.012\n",
      "iter 19100: loss = -89.996\n",
      "iter 19200: loss = -88.603\n",
      "iter 19300: loss = -88.202\n",
      "iter 19400: loss = -88.042\n",
      "iter 19500: loss = -86.781\n",
      "iter 19600: loss = -87.841\n",
      "iter 19700: loss = -91.323\n",
      "iter 19800: loss = -89.458\n",
      "iter 19900: loss = -87.872\n",
      "iter 20000: loss = -88.262\n",
      "iter 20100: loss = -91.201\n",
      "iter 20200: loss = -90.234\n",
      "iter 20300: loss = -92.237\n",
      "iter 20400: loss = -89.147\n",
      "iter 20500: loss = -90.182\n",
      "iter 20600: loss = -87.248\n",
      "iter 20700: loss = -89.703\n",
      "iter 20800: loss = -88.369\n",
      "iter 20900: loss = -92.184\n",
      "iter 21000: loss = -88.504\n",
      "iter 21100: loss = -89.456\n",
      "iter 21200: loss = -88.454\n",
      "iter 21300: loss = -90.300\n",
      "iter 21400: loss = -92.176\n",
      "iter 21500: loss = -93.426\n",
      "iter 21600: loss = -90.679\n",
      "iter 21700: loss = -88.391\n",
      "iter 21800: loss = -90.966\n",
      "iter 21900: loss = -93.520\n",
      "iter 22000: loss = -91.823\n",
      "iter 22100: loss = -91.005\n",
      "iter 22200: loss = -91.035\n",
      "iter 22300: loss = -92.773\n",
      "iter 22400: loss = -88.455\n",
      "iter 22500: loss = -89.398\n",
      "iter 22600: loss = -90.845\n",
      "iter 22700: loss = -92.848\n",
      "iter 22800: loss = -93.833\n",
      "iter 22900: loss = -92.122\n",
      "iter 23000: loss = -93.831\n",
      "iter 23100: loss = -92.780\n",
      "iter 23200: loss = -92.115\n",
      "iter 23300: loss = -92.613\n",
      "iter 23400: loss = -95.029\n",
      "iter 23500: loss = -92.085\n",
      "iter 23600: loss = -91.287\n",
      "iter 23700: loss = -92.879\n",
      "iter 23800: loss = -93.899\n",
      "iter 23900: loss = -92.245\n",
      "iter 24000: loss = -91.299\n",
      "iter 24100: loss = -88.037\n",
      "iter 24200: loss = -87.031\n",
      "iter 24300: loss = -87.600\n",
      "iter 24400: loss = -94.162\n",
      "iter 24500: loss = -94.501\n",
      "iter 24600: loss = -94.970\n",
      "iter 24700: loss = -91.515\n",
      "iter 24800: loss = -92.025\n",
      "iter 24900: loss = -83.694\n",
      "iter 25000: loss = -71.499\n",
      "iter 25100: loss = -87.578\n",
      "iter 25200: loss = -86.592\n",
      "iter 25300: loss = -92.919\n",
      "iter 25400: loss = -91.426\n",
      "iter 25500: loss = -95.493\n",
      "iter 25600: loss = -93.831\n",
      "iter 25700: loss = -93.483\n",
      "iter 25800: loss = -95.965\n",
      "iter 25900: loss = -95.333\n",
      "iter 26000: loss = -94.874\n",
      "iter 26100: loss = -88.928\n",
      "iter 26200: loss = -91.619\n",
      "iter 26300: loss = -93.250\n",
      "iter 26400: loss = -90.460\n",
      "iter 26500: loss = -94.886\n",
      "iter 26600: loss = -94.768\n",
      "iter 26700: loss = -94.445\n",
      "iter 26800: loss = -94.450\n",
      "iter 26900: loss = -95.709\n",
      "iter 27000: loss = -93.697\n",
      "iter 27100: loss = -95.553\n",
      "iter 27200: loss = -95.790\n",
      "iter 27300: loss = -94.334\n",
      "iter 27400: loss = -95.627\n",
      "iter 27500: loss = -95.833\n",
      "iter 27600: loss = -94.407\n",
      "iter 27700: loss = -94.137\n",
      "iter 27800: loss = -94.292\n",
      "iter 27900: loss = -94.523\n",
      "iter 28000: loss = -86.752\n",
      "iter 28100: loss = -94.087\n",
      "iter 28200: loss = -93.391\n",
      "iter 28300: loss = -93.118\n",
      "iter 28400: loss = -96.536\n",
      "iter 28500: loss = -96.192\n",
      "iter 28600: loss = -96.657\n",
      "iter 28700: loss = -93.468\n",
      "iter 28800: loss = -97.146\n",
      "iter 28900: loss = -97.411\n",
      "iter 29000: loss = -95.146\n",
      "iter 29100: loss = -94.878\n",
      "iter 29200: loss = -97.174\n",
      "iter 29300: loss = -92.667\n",
      "iter 29400: loss = -90.223\n",
      "iter 29500: loss = -97.038\n",
      "iter 29600: loss = -96.010\n",
      "iter 29700: loss = -97.322\n",
      "iter 29800: loss = -97.590\n",
      "iter 29900: loss = -89.923\n",
      "iter 30000: loss = -79.974\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam([p for p in network.parameters() if p.requires_grad==True], lr=1e-4)\n",
    "trainloader = data.DataLoader(dataset=training_set, batch_size=128)\n",
    "\n",
    "losses = []\n",
    "avg_energy = []\n",
    "t = 0 \n",
    "for epoch in range(300):\n",
    "    for batch in trainloader:  \n",
    "        z, log_det_J = network.f(batch)\n",
    "              \n",
    "        loss = network.loss_ml(batch)\n",
    "        losses.append(loss.item()) # save values for plotting later \n",
    "        \n",
    "        avg_energy.append(network.expected_value(network.energies, training_set))\n",
    "\n",
    "        optimizer.zero_grad() # we need to set the gradients to zero before starting to do \n",
    "                              # backpropragation because PyTorch accumulates the gradients on \n",
    "                              # subsequent backward passes.\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        t = t + 1 # iteration count\n",
    "\n",
    "        if t % 100 == 0:\n",
    "            print('iter %s:' % t, 'loss = %.3f' % loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next train by both example and by energy \n",
    "Minimizing linear combination of $J_{ML}$ and $J_{KL}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 100: loss = 49.136\n",
      "iter 200: loss = -5.507\n",
      "iter 300: loss = -45.883\n",
      "iter 400: loss = -69.317\n",
      "iter 500: loss = -97.285\n",
      "iter 600: loss = -112.007\n",
      "iter 700: loss = -119.569\n",
      "iter 800: loss = -122.931\n",
      "iter 900: loss = -125.471\n",
      "iter 1000: loss = -128.172\n",
      "iter 1100: loss = -131.148\n",
      "iter 1200: loss = -134.297\n",
      "iter 1300: loss = -137.528\n",
      "iter 1400: loss = -140.794\n",
      "iter 1500: loss = -144.054\n",
      "iter 1600: loss = -147.040\n",
      "iter 1700: loss = -149.371\n",
      "iter 1800: loss = -150.939\n",
      "iter 1900: loss = -151.917\n",
      "iter 2000: loss = -152.547\n",
      "iter 2100: loss = -153.007\n",
      "iter 2200: loss = -153.397\n",
      "iter 2300: loss = -153.766\n",
      "iter 2400: loss = -154.132\n",
      "iter 2500: loss = -154.491\n",
      "iter 2600: loss = -154.828\n",
      "iter 2700: loss = -155.121\n",
      "iter 2800: loss = -155.344\n",
      "iter 2900: loss = -155.499\n",
      "iter 3000: loss = -155.610\n",
      "iter 3100: loss = -155.693\n",
      "iter 3200: loss = -155.760\n",
      "iter 3300: loss = -155.817\n",
      "iter 3400: loss = -155.866\n",
      "iter 3500: loss = -155.910\n",
      "iter 3600: loss = -155.950\n",
      "iter 3700: loss = -155.987\n",
      "iter 3800: loss = -156.021\n",
      "iter 3900: loss = -156.055\n",
      "iter 4000: loss = -156.086\n",
      "iter 4100: loss = -156.117\n",
      "iter 4200: loss = -156.146\n",
      "iter 4300: loss = -156.175\n",
      "iter 4400: loss = -156.203\n",
      "iter 4500: loss = -156.230\n",
      "iter 4600: loss = -156.257\n",
      "iter 4700: loss = -156.283\n",
      "iter 4800: loss = -156.309\n",
      "iter 4900: loss = -156.334\n",
      "iter 5000: loss = -156.360\n",
      "iter 5100: loss = -156.385\n",
      "iter 5200: loss = -156.409\n",
      "iter 5300: loss = -156.434\n",
      "iter 5400: loss = -156.458\n",
      "iter 5500: loss = -156.483\n",
      "iter 5600: loss = -156.508\n",
      "iter 5700: loss = -156.533\n",
      "iter 5800: loss = -156.558\n",
      "iter 5900: loss = -156.583\n",
      "iter 6000: loss = -156.608\n",
      "iter 6100: loss = -156.631\n",
      "iter 6200: loss = -156.652\n",
      "iter 6300: loss = -156.670\n",
      "iter 6400: loss = -156.683\n",
      "iter 6500: loss = -156.693\n",
      "iter 6600: loss = -156.699\n",
      "iter 6700: loss = -156.703\n",
      "iter 6800: loss = -156.707\n",
      "iter 6900: loss = -156.709\n",
      "iter 7000: loss = -156.711\n",
      "iter 7100: loss = -156.713\n",
      "iter 7200: loss = -156.714\n",
      "iter 7300: loss = -156.715\n",
      "iter 7400: loss = -156.717\n",
      "iter 7500: loss = -156.718\n",
      "iter 7600: loss = -156.719\n",
      "iter 7700: loss = -156.720\n",
      "iter 7800: loss = -156.720\n",
      "iter 7900: loss = -156.721\n",
      "iter 8000: loss = -156.722\n",
      "iter 8100: loss = -156.723\n",
      "iter 8200: loss = -156.723\n",
      "iter 8300: loss = -156.724\n",
      "iter 8400: loss = -156.724\n",
      "iter 8500: loss = -156.725\n",
      "iter 8600: loss = -156.725\n",
      "iter 8700: loss = -156.726\n",
      "iter 8800: loss = -156.726\n",
      "iter 8900: loss = -156.727\n",
      "iter 9000: loss = -156.727\n",
      "iter 9100: loss = -156.727\n",
      "iter 9200: loss = -156.728\n",
      "iter 9300: loss = -156.728\n",
      "iter 9400: loss = -156.729\n",
      "iter 9500: loss = -156.729\n",
      "iter 9600: loss = -156.730\n",
      "iter 9700: loss = -156.730\n",
      "iter 9800: loss = -156.730\n",
      "iter 9900: loss = -156.731\n",
      "iter 10000: loss = -156.731\n",
      "iter 10100: loss = -156.731\n",
      "iter 10200: loss = -156.732\n",
      "iter 10300: loss = -156.732\n",
      "iter 10400: loss = -156.732\n",
      "iter 10500: loss = -156.733\n",
      "iter 10600: loss = -156.733\n",
      "iter 10700: loss = -156.733\n",
      "iter 10800: loss = -156.734\n",
      "iter 10900: loss = -156.734\n",
      "iter 11000: loss = -156.735\n",
      "iter 11100: loss = -156.735\n",
      "iter 11200: loss = -156.735\n",
      "iter 11300: loss = -156.736\n",
      "iter 11400: loss = -156.736\n",
      "iter 11500: loss = -156.736\n",
      "iter 11600: loss = -156.737\n",
      "iter 11700: loss = -156.737\n",
      "iter 11800: loss = -156.738\n",
      "iter 11900: loss = -156.738\n",
      "iter 12000: loss = -156.738\n",
      "iter 12100: loss = -156.739\n",
      "iter 12200: loss = -156.739\n",
      "iter 12300: loss = -156.739\n",
      "iter 12400: loss = -156.740\n",
      "iter 12500: loss = -156.740\n",
      "iter 12600: loss = -156.740\n",
      "iter 12700: loss = -156.740\n",
      "iter 12800: loss = -156.741\n",
      "iter 12900: loss = -156.741\n",
      "iter 13000: loss = -156.741\n",
      "iter 13100: loss = -156.742\n",
      "iter 13200: loss = -156.742\n",
      "iter 13300: loss = -156.742\n",
      "iter 13400: loss = -156.743\n",
      "iter 13500: loss = -156.743\n",
      "iter 13600: loss = -156.743\n",
      "iter 13700: loss = -156.743\n",
      "iter 13800: loss = -156.744\n",
      "iter 13900: loss = -156.744\n",
      "iter 14000: loss = -156.744\n",
      "iter 14100: loss = -156.745\n",
      "iter 14200: loss = -156.745\n",
      "iter 14300: loss = -156.745\n",
      "iter 14400: loss = -156.745\n",
      "iter 14500: loss = -156.746\n",
      "iter 14600: loss = -156.746\n",
      "iter 14700: loss = -156.746\n",
      "iter 14800: loss = -156.747\n",
      "iter 14900: loss = -156.747\n",
      "iter 15000: loss = -156.747\n",
      "iter 15100: loss = -156.748\n",
      "iter 15200: loss = -156.748\n",
      "iter 15300: loss = -156.748\n",
      "iter 15400: loss = -156.749\n",
      "iter 15500: loss = -156.749\n",
      "iter 15600: loss = -156.749\n",
      "iter 15700: loss = -156.750\n",
      "iter 15800: loss = -156.750\n",
      "iter 15900: loss = -156.750\n",
      "iter 16000: loss = -156.751\n",
      "iter 16100: loss = -156.751\n",
      "iter 16200: loss = -156.751\n",
      "iter 16300: loss = -156.752\n",
      "iter 16400: loss = -156.752\n",
      "iter 16500: loss = -156.752\n",
      "iter 16600: loss = -156.753\n",
      "iter 16700: loss = -156.753\n",
      "iter 16800: loss = -156.753\n",
      "iter 16900: loss = -156.754\n",
      "iter 17000: loss = -156.754\n",
      "iter 17100: loss = -156.755\n",
      "iter 17200: loss = -156.755\n",
      "iter 17300: loss = -156.755\n",
      "iter 17400: loss = -156.756\n",
      "iter 17500: loss = -156.756\n",
      "iter 17600: loss = -156.756\n",
      "iter 17700: loss = -156.756\n",
      "iter 17800: loss = -156.757\n",
      "iter 17900: loss = -156.757\n",
      "iter 18000: loss = -156.757\n",
      "iter 18100: loss = -156.758\n",
      "iter 18200: loss = -156.758\n",
      "iter 18300: loss = -156.758\n",
      "iter 18400: loss = -156.758\n",
      "iter 18500: loss = -156.759\n",
      "iter 18600: loss = -156.759\n",
      "iter 18700: loss = -156.759\n",
      "iter 18800: loss = -156.759\n",
      "iter 18900: loss = -156.760\n",
      "iter 19000: loss = -156.760\n",
      "iter 19100: loss = -156.760\n",
      "iter 19200: loss = -156.761\n",
      "iter 19300: loss = -156.761\n",
      "iter 19400: loss = -156.761\n",
      "iter 19500: loss = -156.761\n",
      "iter 19600: loss = -156.761\n",
      "iter 19700: loss = -156.762\n",
      "iter 19800: loss = -156.762\n",
      "iter 19900: loss = -156.762\n",
      "iter 20000: loss = -156.762\n",
      "iter 20100: loss = -156.763\n",
      "iter 20200: loss = -156.763\n",
      "iter 20300: loss = -156.763\n",
      "iter 20400: loss = -156.764\n",
      "iter 20500: loss = -156.764\n",
      "iter 20600: loss = -156.764\n",
      "iter 20700: loss = -156.764\n",
      "iter 20800: loss = -156.765\n",
      "iter 20900: loss = -156.765\n",
      "iter 21000: loss = -156.765\n",
      "iter 21100: loss = -156.765\n",
      "iter 21200: loss = -156.765\n",
      "iter 21300: loss = -156.765\n",
      "iter 21400: loss = -156.765\n",
      "iter 21500: loss = -156.767\n",
      "iter 21600: loss = -156.768\n",
      "iter 21700: loss = -156.765\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam([p for p in network.parameters() if p.requires_grad==True], lr=1e-4)\n",
    "\n",
    "s = 0.1 # Metropolis Monte Carlo step size \n",
    "m = distributions.Uniform(torch.tensor([0.0]), torch.tensor([1.0])) # uniform distribution\n",
    "\n",
    "t = 0\n",
    "for epoch in range(500): \n",
    "    trainloader = data.DataLoader(dataset=training_set, batch_size=128)\n",
    "    for batch in trainloader:   \n",
    "\n",
    "        loss = network.loss_ml(batch) + network.loss_kl(batch)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        avg_energy.append(network.expected_value(network.energies, training_set))\n",
    "\n",
    "        optimizer.zero_grad() # we need to set the gradients to zero before starting to do \n",
    "                              # backpropragation because PyTorch accumulates the gradients on \n",
    "                              # subsequent backward passes.\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        t = t + 1 # iteration count\n",
    "\n",
    "        # Metropolis Monte Carlo step (almost completely vectorized) \n",
    "#         z, log_R_xz = network.f(batch)\n",
    "#         z_prop = z + s*prior.sample((z.shape[0],)) # proposed move in latent space\n",
    "#         x_prop, log_R_zx = network.g(z_prop) # corresponding position in real space\n",
    "        \n",
    "#         delta_E = torch.zeros_like(log_R_xz)\n",
    "#         for i in range(batch.shape[0]): # for each x in the batch \n",
    "#             delta_E[i] = ising.energy(x_prop[i,:].reshape((N,N))) - ising.energy(batch[i,:].reshape((N,N))) - log_R_zx[i] + log_R_xz[i]\n",
    "#         acceptance_ratio = torch.min(torch.ones(delta_E.shape), torch.exp(-delta_E))\n",
    "#         mask = torch.rand(acceptance_ratio.shape) <= acceptance_ratio\n",
    "        \n",
    "#         for i in range(batch.shape[0]): # for each x in the batch \n",
    "#             if mask[i]: # accept candidate \n",
    "#                 training_set[i,:] = x_prop[i,:]\n",
    "\n",
    "        if t % 100 == 0:\n",
    "            print('iter %s:' % t, 'loss = %.3f' % loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debugging trail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(ising.energy(x_prop[i,:].reshape((N,N)))) # none?\n",
    "# print(-ising.energy(batch[i,:].reshape((N,N)))) # works fine \n",
    "# print( - log_R_zx[i]) # NaN\n",
    "# print(log_R_xz[i]) # NaN\n",
    "# print(x_prop) # NaN\n",
    "# print(z_prop) # NaN\n",
    "# print(z) # NaN\n",
    "print(network.f(batch)) # NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results\n",
    "### View loss as a function of iteration steps\n",
    "Training should proceed until the loss approximately levels out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = go.Figure() # plotly reference: https://plot.ly/python/line-charts/\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(losses)), y=losses,\n",
    "                    mode='lines',\n",
    "                    name='lines'))\n",
    "# Edit the layout\n",
    "fig.update_layout(yaxis_title='Loss',\n",
    "                   xaxis_title='Iteration #')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View average energy as a function of iteration steps\n",
    "We should see the average energy decrease once we perform adaptive sampling during training by energy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = go.Figure() # plotly reference: https://plot.ly/python/line-charts/\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(losses)), y=avg_energy,\n",
    "                    mode='lines',\n",
    "                    name='lines'))\n",
    "# Edit the layout\n",
    "fig.update_layout(yaxis_title='Avg. Energy',\n",
    "                   xaxis_title='Iteration #')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
